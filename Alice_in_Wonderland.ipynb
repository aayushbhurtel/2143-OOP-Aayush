{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMNmf76hY1G/noiAWsLw1D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aayushbhurtel/2143-OOP-Aayush/blob/main/Alice_in_Wonderland.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import re\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "\n",
        "DATA_DIR = \"./data\"\n",
        "CHECKPOINT_DIR = os.path.join(DATA_DIR, \"checkpoints\")\n",
        "LOG_DIR = os.path.join(DATA_DIR, \"logs\")\n",
        "\n",
        "\n",
        "def clean_logs():\n",
        "    shutil.rmtree(CHECKPOINT_DIR, ignore_errors=True)\n",
        "    shutil.rmtree(LOG_DIR, ignore_errors=True)\n",
        "\n",
        "\n",
        "def download_and_read(urls):\n",
        "    texts = []\n",
        "    for i, url in enumerate(urls):\n",
        "        p = tf.keras.utils.get_file(\"ex1-{:d}.txt\".format(i), url,\n",
        "            cache_dir=\".\")\n",
        "        text = open(p, mode=\"r\", encoding=\"utf-8\").read()\n",
        "        # remove byte order mark\n",
        "        text = text.replace(\"\\ufeff\", \"\")\n",
        "        # remove newlines\n",
        "        text = text.replace('\\n', ' ')\n",
        "        text = re.sub(r'\\s+', \" \", text)\n",
        "        # add it to the list\n",
        "        texts.extend(text)\n",
        "    return texts\n",
        "\n",
        "\n",
        "def split_train_labels(sequence):\n",
        "    input_seq = sequence[0:-1]\n",
        "    output_seq = sequence[1:]\n",
        "    return input_seq, output_seq\n",
        "\n",
        "\n",
        "class CharGenModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, vocab_size, num_timesteps,\n",
        "            embedding_dim, **kwargs):\n",
        "        super(CharGenModel, self).__init__(**kwargs)\n",
        "        self.embedding_layer = tf.keras.layers.Embedding(\n",
        "            vocab_size,\n",
        "            embedding_dim\n",
        "        )\n",
        "        self.rnn_layer = tf.keras.layers.GRU(\n",
        "            num_timesteps,\n",
        "            recurrent_initializer=\"glorot_uniform\",\n",
        "            recurrent_activation=\"sigmoid\",\n",
        "            stateful=True,\n",
        "            return_sequences=True\n",
        "        )\n",
        "        self.dense_layer = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.embedding_layer(x)\n",
        "        x = self.rnn_layer(x)\n",
        "        x = self.dense_layer(x)\n",
        "        return x\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super().build(input_shape)\n",
        "        self.embedding_layer.build(input_shape)\n",
        "        self.rnn_layer.build(self.embedding_layer.compute_output_shape(input_shape))\n",
        "        self.dense_layer.build(self.rnn_layer.compute_output_shape(\n",
        "            self.embedding_layer.compute_output_shape(input_shape)\n",
        "        ))\n",
        "\n",
        "\n",
        "def loss(labels, predictions):\n",
        "    return tf.losses.sparse_categorical_crossentropy(\n",
        "        labels,\n",
        "        predictions,\n",
        "        from_logits=True\n",
        "    )\n",
        "\n",
        "\n",
        "def generate_text(model, prefix_string, char2idx, idx2char,\n",
        "        num_chars_to_generate=1000, temperature=1.0):\n",
        "    input = [char2idx[s] for s in prefix_string]\n",
        "    input = tf.expand_dims(input, 0)\n",
        "    text_generated = []\n",
        "    model.rnn_layer.reset_states() #### changed\n",
        "    for i in range(num_chars_to_generate):\n",
        "        preds = model(input)\n",
        "        preds = tf.squeeze(preds, 0) / temperature\n",
        "        # predict char returned by model\n",
        "        pred_id = tf.random.categorical(preds, num_samples=1)[-1, 0].numpy()\n",
        "        text_generated.append(idx2char[pred_id])\n",
        "        # pass the prediction as the next input to the model\n",
        "        input = tf.expand_dims([pred_id], 0)\n",
        "\n",
        "    return prefix_string + \"\".join(text_generated)\n",
        "\n",
        "\n",
        "# download and read into local data structure (list of chars)\n",
        "texts = download_and_read([\n",
        "    \"http://www.gutenberg.org/cache/epub/28885/pg28885.txt\",\n",
        "    \"https://www.gutenberg.org/files/12/12-0.txt\"\n",
        "])\n",
        "clean_logs()\n",
        "\n",
        "# create the vocabulary\n",
        "vocab = sorted(set(texts))\n",
        "print(\"vocab size: {:d}\".format(len(vocab)))\n",
        "\n",
        "# create mapping from vocab chars to ints\n",
        "char2idx = {c:i for i, c in enumerate(vocab)}\n",
        "idx2char = {i:c for c, i in char2idx.items()}\n",
        "\n",
        "# numericize the texts\n",
        "texts_as_ints = np.array([char2idx[c] for c in texts])\n",
        "data = tf.data.Dataset.from_tensor_slices(texts_as_ints)\n",
        "\n",
        "# number of characters to show before asking for prediction\n",
        "# sequences: [None, 100]\n",
        "seq_length = 100\n",
        "sequences = data.batch(seq_length + 1, drop_remainder=True)\n",
        "sequences = sequences.map(split_train_labels)\n",
        "\n",
        "# print out input and output to see what they look like\n",
        "for input_seq, output_seq in sequences.take(1):\n",
        "    print(\"input:[{:s}]\".format(\n",
        "        \"\".join([idx2char[i] for i in input_seq.numpy()])))\n",
        "    print(\"output:[{:s}]\".format(\n",
        "        \"\".join([idx2char[i] for i in output_seq.numpy()])))\n",
        "\n",
        "# set up for training\n",
        "# batches: [None, 64, 100]\n",
        "batch_size = 64\n",
        "steps_per_epoch = len(texts) // seq_length // batch_size\n",
        "dataset = sequences.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
        "print(dataset)\n",
        "\n",
        "# define network\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "\n",
        "model = CharGenModel(vocab_size, seq_length, embedding_dim)\n",
        "model.build(input_shape=(batch_size, seq_length))\n",
        "model.summary()\n",
        "\n",
        "# try running some data through the model to validate dimensions\n",
        "for input_batch, label_batch in dataset.take(1):\n",
        "    pred_batch = model(input_batch)\n",
        "\n",
        "print(pred_batch.shape)\n",
        "assert(pred_batch.shape[0] == batch_size)\n",
        "assert(pred_batch.shape[1] == seq_length)\n",
        "assert(pred_batch.shape[2] == vocab_size)\n",
        "\n",
        "model.compile(optimizer=tf.optimizers.Adam(), loss=loss)\n",
        "\n",
        "# we will train our model for 50 epochs, and after every 10 epochs\n",
        "# we want to see how well it will generate text\n",
        "num_epochs = 50\n",
        "\n",
        "# Create the checkpoint directory if it doesn't exist\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "for i in range(num_epochs // 10):\n",
        "    model.fit(\n",
        "        dataset.repeat(),\n",
        "        epochs=10,\n",
        "        steps_per_epoch=steps_per_epoch\n",
        "        # callbacks=[checkpoint_callback, tensorboard_callback]\n",
        "    )\n",
        "    checkpoint_file = os.path.join(\n",
        "        CHECKPOINT_DIR, \"model_epoch_{:d}.weights.h5\".format(i+1))\n",
        "    model.save_weights(checkpoint_file)\n",
        "\n",
        "    # create a generative model using the trained model so far\n",
        "    gen_model = CharGenModel(vocab_size, seq_length, embedding_dim)\n",
        "\n",
        "    gen_model.build(input_shape=(1, seq_length))\n",
        "    gen_model.load_weights(checkpoint_file)\n",
        "    print(\"after epoch: {:d}\".format((i+1)*10))\n",
        "    print(generate_text(gen_model, \"Alice \", char2idx, idx2char))\n",
        "    print(\"---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "P2_aYmFQGNDD",
        "outputId": "481cad01-0f68-4126-81b4-510d44c11b91"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 93\n",
            "input:[The Project Gutenberg eBook of Alice's Adventures in Wonderland This ebook is for the use of anyone ]\n",
            "output:[he Project Gutenberg eBook of Alice's Adventures in Wonderland This ebook is for the use of anyone a]\n",
            "<_BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"char_gen_model_17\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"char_gen_model_17\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_17 (\u001b[38;5;33mEmbedding\u001b[0m)        │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │        \u001b[38;5;34m23,808\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_17 (\u001b[38;5;33mGRU\u001b[0m)                    │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)         │       \u001b[38;5;34m107,400\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m93\u001b[0m)          │         \u001b[38;5;34m9,393\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">23,808</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                    │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">107,400</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,393</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m140,601\u001b[0m (549.22 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">140,601</span> (549.22 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m140,601\u001b[0m (549.22 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">140,601</span> (549.22 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 93)\n",
            "Epoch 1/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 306ms/step - loss: 3.6496\n",
            "Epoch 2/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 331ms/step - loss: 2.5239\n",
            "Epoch 3/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 306ms/step - loss: 2.3197\n",
            "Epoch 4/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 306ms/step - loss: 2.1798\n",
            "Epoch 5/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 304ms/step - loss: 2.0756\n",
            "Epoch 6/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 308ms/step - loss: 2.0009\n",
            "Epoch 7/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 326ms/step - loss: 1.9414\n",
            "Epoch 8/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 304ms/step - loss: 1.8972\n",
            "Epoch 9/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 306ms/step - loss: 1.8383\n",
            "Epoch 10/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 302ms/step - loss: 1.8083\n",
            "after epoch: 10\n",
            "Alice quecs,” A criks. Whreasing 1.-0RÆ4. Be waud up deepto’s had it so ratton on. “Turlistihe soo could not rould, 1ices!\" your that tower the and ullared beat time. Dirutpolf a rucer that Hcime on, thinshe it'w them I well ceepnor: reatile hassed, very that se groveall broughther some!’ The Rent of seon whind fish that wand'n you she said _aning cot’s maidy-fird cress main those, as cabling he rinush cred terectly ame of roully inely, fitild ohe have is af gote som of his pat-f \"itmrayeds I gook no remppid over into?” “I's very in the ut so ware any Queen xied the ding tha pely to that little anuth one iel All it all by thar till a forse maden thear phovight; you?\" She I wand, \"seed then gurt the on. “Lely Unice'.\" She t litted a tlen, and on mares old,\" said the pamed and a whatrersulived Cane Hattend umpayter she sing: and. \"I’m you to like dras _been, and Wly lick The toself,\" Doibut to _southed she madet tron'ving evough yes don.” “And only iss, as shree “Shall. “Chatenter drook owalin\n",
            "---\n",
            "Epoch 1/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 302ms/step - loss: 1.7702\n",
            "Epoch 2/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 300ms/step - loss: 1.7390\n",
            "Epoch 3/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 333ms/step - loss: 1.7119\n",
            "Epoch 4/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 307ms/step - loss: 1.6834\n",
            "Epoch 5/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 306ms/step - loss: 1.6687\n",
            "Epoch 6/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 314ms/step - loss: 1.6467\n",
            "Epoch 7/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 306ms/step - loss: 1.6292\n",
            "Epoch 8/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 331ms/step - loss: 1.6122\n",
            "Epoch 9/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 304ms/step - loss: 1.5975\n",
            "Epoch 10/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 302ms/step - loss: 1.5815\n",
            "after epoch: 20\n",
            "Alice wrigh with eace suddout thow in the ghings what’s at as in here tone. At live it come to her to lick a read went on out and only. \"Sumpriled, oo very all froct in tonef yor had wwith is wofitee doise Alice wondered light gown this sharm ‘_I'm! There thought DoE Thaling with afferes and thergumer) “you know, thowe in expidbine!\" on her nu hold on qued her sax in let slaw—they the Mock and ondiouthel, \"I'r do began anly, and pasted Turn't hadded is forline morak everying _neverr!\" Better thearNorthing to cazles, is, plos withinges nover yourgener to feever (had Alice—but to be into the doon,” the morave they with a said, \"I did hand of to the paraving very King-roment for as nos_ about to the lito and stal I don't real?” “adden is-queer. “Alicew use Or down a quile,s greatchillarlo“there eng of pecked Alice. \"\"I himptif---'se won't doo?” Haught they panatious the yough the rouse. What she Queen down sall thingse at the King writh I glartens in the tone had nieved at your as mind make ove\n",
            "---\n",
            "Epoch 1/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 303ms/step - loss: 1.5662\n",
            "Epoch 2/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 301ms/step - loss: 1.5572\n",
            "Epoch 3/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 303ms/step - loss: 1.5439\n",
            "Epoch 4/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 323ms/step - loss: 1.5320\n",
            "Epoch 5/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 308ms/step - loss: 1.5222\n",
            "Epoch 6/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 299ms/step - loss: 1.5127\n",
            "Epoch 7/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 304ms/step - loss: 1.5138\n",
            "Epoch 8/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 301ms/step - loss: 1.5016\n",
            "Epoch 9/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 302ms/step - loss: 1.4915\n",
            "Epoch 10/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 320ms/step - loss: 1.4816\n",
            "after epoch: 30\n",
            "Alice remound a creepener than instrock-had not a whit hersely his no Alice! In pabpitiony. “Do anur such him,\" said Alice, and said, “I for there take ou. And wook bjun?” “I don’t had exchibling as fartly wasn’t siganing in mays surposess— they was you satcabidd you’t might cold her. “Don’t intage again, but I read, Alied, it for the round some hode,” the Duch!, there’s words if she, who hances and was eathing all sleet ant forreat loud,\" said their scopence of no verysh interalarf_, it would I’m naming the backed too the ed--Gail, and went ape chang about the srounto you, that preager ‘8.D. Yealless thinking a if you she fasse is, he seat! At _this_s_—I dare_ have dasted ham voliout me ormbling, all butting that wouldered to the find to white unnling again, course to gelf of you she looked fell which Mocknted, so is_—” “What room and doon, and was out. Alice poinsice of thinking, and mak again: \"no in the hanth’” thought explain. “I don’t look from sometailterand. Eng trided Panction and g\n",
            "---\n",
            "Epoch 1/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 319ms/step - loss: 1.4818\n",
            "Epoch 2/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 300ms/step - loss: 1.4733\n",
            "Epoch 3/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 303ms/step - loss: 1.4592\n",
            "Epoch 4/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 301ms/step - loss: 1.4578\n",
            "Epoch 5/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 302ms/step - loss: 1.4598\n",
            "Epoch 6/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 308ms/step - loss: 1.4462\n",
            "Epoch 7/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 319ms/step - loss: 1.4430\n",
            "Epoch 8/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 299ms/step - loss: 1.4374\n",
            "Epoch 9/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 301ms/step - loss: 1.4417\n",
            "Epoch 10/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 299ms/step - loss: 1.4229\n",
            "after epoch: 40\n",
            "Alice went lifted lost on, and wh off thim” \"I plawy, or tried down Alice on that. Alice hadn’t having as were share great quite have sounds of eep, howl in Queen Frong, us hall. He; but then fain,” said Alice, is wondernother rom.\" \"Heardo!\" and a shar! And why at as I never hardly kittle few seet off by by the havese; \"and white mad! And drandle more. I think of books!\" Who learket, exad in the bland of it is the end of it in lives you temply! and don’t’s 'But.” Herpoidors have go!” Alice could noting it. “That_?\" sighter having?\" a little oop and saze and my as the large oblice-now, hormal gettink the have been yrieed cut about, and happench as she mome, some hands of plassed to he went away,\" the Med Queen quite--opled, as she past!\" The Dormouse should hawn say,\" said Alice: and Waling twife _you_ criecusat at the may is!” (Alice added in a slees as it argrecorna notice where and come!” (Lividdwe shoes to chiling that’s angried it would be drawl the should half no both 31 wasnation _ame\n",
            "---\n",
            "Epoch 1/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 302ms/step - loss: 1.4243\n",
            "Epoch 2/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 303ms/step - loss: 1.4232\n",
            "Epoch 3/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 303ms/step - loss: 1.4163\n",
            "Epoch 4/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 336ms/step - loss: 1.4152\n",
            "Epoch 5/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 301ms/step - loss: 1.4106\n",
            "Epoch 6/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 306ms/step - loss: 1.4048\n",
            "Epoch 7/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 303ms/step - loss: 1.4090\n",
            "Epoch 8/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 314ms/step - loss: 1.4034\n",
            "Epoch 9/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 331ms/step - loss: 1.3990\n",
            "Epoch 10/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 305ms/step - loss: 1.3901\n",
            "after epoch: 50\n",
            "Alice many don’t I met) Tweedled, they as so be beet (Another, “not shoments at lovebles this work you do an broke iveral quite arm. Willieme the pryou sail of a said, very by; and a grinning this memant to hand douse, “Who is the pracce.\" The Queen said. “Loctle want and at her here. CHAPOR. Pressice. •.Y It was to the wermorand lifder voice—little of my bits inshest Divildly run keath and withing to herseling wish the satched for, you very came acciled to Alice, was some off with watch of the wanders and of flame up intch care, Tition--Whole, but I wish the croes“Alice wear child us of the learme looked coudery!” Alice reted the back out out up--Farled about eBook guesidat eir herself, I'm very bext leaves short child crashay.” “Now what a minuted to for bey: though nice. \"Went there queen te adve to over, and the pleasuging of by the willing, as she cers in a to-dracmunquerat had not some of the reparnation the nibece she oppeeds with mant and this provide,\" said Alice diretting the Queen\n",
            "---\n"
          ]
        }
      ]
    }
  ]
}